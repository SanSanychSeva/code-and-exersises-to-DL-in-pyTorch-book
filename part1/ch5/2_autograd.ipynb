{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# вернемся к разложению градиента функции потерь по весам на модельную часть и часть зависящую от функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_w L_{oss} = J_{model}^T \\,\\cdot\\, \\nabla_{\\hat y} L_{oss}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## матрица Якоби определяется только моделью - это линейная часть зависимости вектора ответов $\\hat y$ от вектора переметров $w$, включая интерсепт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "d\\hat y \\equiv J_{model} \\cdot dw \\,\\,\\,\\Rightarrow\\,\\,\\, J^{model}_{ji} \\equiv \\left(\\frac{\\partial\\hat y_j}{\\partial w_i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## градиент же функции потерь по ответам модели определяется только формулой функции потерь и не зависит от весов и модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\hat y} L_{oss} = \\frac{2}{n}\\, (\\hat y - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**если функция потерь - MSE, то этот градиент в пространстве ответов пропорционален ошибке ответа на разметке, поэтому формула итеративного обновления весов в GD называется алгоритмом обратного распространения ошибки: в прямом проходе мы вычисляем новые ошибки - используя текущие веса модели, в обратном проходе по ошибкам мы линейно обновляем веса:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w' = w - \\frac{2\\delta}{n}\\; J_{model}^T \\,\\cdot\\, (\\hat y - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# имея шаговые инкременты весов и ответов, можно ли параллельно оценивать градиент без вывода аналитической формулы?\n",
    "\n",
    "**NB!**: PyTorch tensors can remember where they come from, in terms of the operations and parent tensors that originated them, and they can automatically provide the chain of derivatives of such operations with respect to their inputs. This means we won’t need to derive our model by hand: given a forward expression, no matter how nested, PyTorch will automatically provide the gradient of that expression with respect to its input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## поэкспериментируем с `autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.], requires_grad=True), None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vector = torch.tensor([1.0,1.0], requires_grad=True)\n",
    "x_vector, x_vector.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2., grad_fn=<SumBackward0>), None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scalar = (x_vector**2).sum()\n",
    "y_scalar, x_vector.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scalar.backward()\n",
    "x_vector.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "FYI: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n",
      "--------------------------------------------------------------------------------\n",
      "going via except: tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y_scalar.backward()\n",
    "    print(x_vector.grad)\n",
    "except:\n",
    "    error_msg = traceback.format_exc()\n",
    "    print('-'*80)\n",
    "    print('FYI:', error_msg.split('RuntimeError:')[-1].strip())\n",
    "    print('-'*80)\n",
    "    \n",
    "    x_vector.grad = None\n",
    "    y_scalar = (x_vector**2).sum()\n",
    "    y_scalar.backward()\n",
    "    print('going via except:', x_vector.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## а теперь если функция не скаляр, а вектор?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([\n",
    "    [1.,2.],\n",
    "    [3.,4.],\n",
    "    [5.,6.],\n",
    "    [7.,8.]\n",
    "])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  7., 11., 15.], grad_fn=<MvBackward0>), None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vector.grad = None\n",
    "y_vector = X @ x_vector\n",
    "y_vector, x_vector.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "FYI: grad can be implicitly created only for scalar outputs\n",
      "--------------------------------------------------------------------------------\n",
      "going via except:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y_vector.backward()\n",
    "    print(x_vector.grad)\n",
    "except:\n",
    "    error_msg = traceback.format_exc()\n",
    "    print('-'*80)\n",
    "    print('FYI:', error_msg.split('RuntimeError:')[-1].strip())\n",
    "    print('-'*80)\n",
    "\n",
    "    J = []\n",
    "    for idx in range(y_vector.shape[0]):\n",
    "        x_vector.grad = None\n",
    "        y_vector = X @ x_vector\n",
    "        y_vector[idx].backward()\n",
    "        J.append(x_vector.grad)\n",
    "    J = torch.stack(J)\n",
    "    print('going via except:')\n",
    "    print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица Якоби найдена верно!\n"
     ]
    }
   ],
   "source": [
    "if (X != J).sum() == 0:\n",
    "    print('Матрица Якоби найдена верно!')\n",
    "else:\n",
    "    print('Матрица Якоби неверна!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**заметим, что сообщение об ошибке (а также`MvBackward` вместо `SumBackward` у векторной функции `tensor([ 3.,  7., 11., 15.], grad_fn=<MvBackward0>)`) неявно указывает, что эксплицитно и автоград как-то может находить градиент для векторной функции - надежда избежать повторных вычислений каждой компоненты функции все же есть**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## повторим упражнение с GD без аналитической формулы градиента, но с `autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0,\n",
    "                    3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
    "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, params):\n",
    "    X = torch.stack([torch.ones(t_u.shape[0]), t_u]).T\n",
    "    return X @ params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = tensor(0., grad_fn=<SelectBackward0>) \n",
      "w = tensor(1., grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([0.0, 1.0], requires_grad=True)\n",
    "print('b =', params[0], '\\nw =', params[1])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(params.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### starting loss & autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  82.6000, 4517.2969])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, params), t_c)\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB!: для каждой новой точки $(b,w)$ надо руками завойдить градиент в тензоре параметров перед вычислением функции потерь в ней и вызова autograd-метода `tensor.backward()`\n",
    "при вызове backward производные накапливаются в узлах-листьях. Так что, если backward вызывался ранее, потери оцениваются опять, backward вызывается снова (как и в любом цикле обучения), после чего накапливаются градиенты во всех листьях графа, то есть суммируются с вычисленными на предыдущей итерации, в результате чего получается неправильное значение градиента.\n",
    "\n",
    "Чтобы предотвратить подобное, необходимо явным образом обнулять градиенты на каждой итерации. Это легко сделать с помощью метода с заменой на месте `tensor.grad.zero_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## с учетом всего вышеописанного получим вариант GD-алгоритма с автоградом так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():                                  # чтобы он не дифференцировал новые параметры по старым на шаге итерации\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch', epoch, ': Loss =', float(loss.detach()))   # можно просто выводить loss, но некрасиво - параметр grad_fn, \n",
    "                                                                       # а при любых преобразованиях будет опять растить граф автограда\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500 : Loss = 7.860115051269531\n",
      "Epoch 1000 : Loss = 3.828537940979004\n",
      "Epoch 1500 : Loss = 3.092191219329834\n",
      "Epoch 2000 : Loss = 2.957697868347168\n",
      "Epoch 2500 : Loss = 2.933133840560913\n",
      "Epoch 3000 : Loss = 2.9286484718322754\n",
      "Epoch 3500 : Loss = 2.9278297424316406\n",
      "Epoch 4000 : Loss = 2.9276793003082275\n",
      "Epoch 4500 : Loss = 2.927651882171631\n",
      "Epoch 5000 : Loss = 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-17.3012,   5.3671], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([0.0, 1.0], requires_grad=True),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
